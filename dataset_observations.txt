
"Ta Feng" data set: observations, description of analysis so far, and outlook

DATASET ISSUES (dealt with in preprocess.py):

1)  A few products belong to more than one product subclass. This is likely an error, and in the entire 4 month period effects a significant amount of data (order of 1-2%). See preprocess.py for details on how this is fixed.

2) It is not completely obvious what the ”Assets" column in original data refers to. I think the evidence is sufficient to classify these as values of the items, i.e. "Sales Price" - "Assets" = "Margins".


GENERAL OBSERVATIONS (all referenced plots are generated by visualize.py):

1) Weekly periodicity is obvious. Most shopping is done over weekends, especially on Sundays (see daily transactions and daily sales vs. date bar plots).

2) Special dates, when customer behaviour isn't typical, include Chinese N.Y. 
Wed. 24/01/2001 and Constitution day in Taiwan 25/01/2000 == Christmas. Good to avoid these periods whenever analysis is meant to be generally valid. It is for this reason that "preprocess.py" generates a dataset that drops dates affected by holidays, and also ensures the whole thing is periodic in weeks (the dataset is saved as “all_periodic.csv”; I've not yet had time to make specific use of this dataset).

3) Sales peak for the "35-39" bracket, and decrease almost monotonically to either side - the whole thing roughly resembles a Poisson distribution. Group K is not explained in the original data, and is significant in size (see Age vs. Sales bar chart). 

4 )Sales peak for postcodes closest to supermarket, and roughly decrease with distance - but "other" and "unknown" categories contribute significantly (see Residence vs. Sales bar chart).

5) Sparsity issues:
	a) Many customers visit infrequently (app. 30% only one, see customer visits histogram).
	b) Cheap items sell in large volume, very few sales of many expensive items (see "average item price" and "average price of item in subclass" histograms).
	c) Most often few items are bought per shopping trip - shopping trips involving large numbers of items are scarce (see "Items" bought per visit" histogram).
	d) The "Total no. of sales" vs. "Products (ordered)" plot demonstrates that a products that are frequently purchased account for a large fraction of sales, but for a small fraction of all available products - note that the x-axis is cut off at 2000 in the plot, but the no. of available products is 23805 for the full dataset. In brief: the tails are very long.

6) The reason why I believe that the "Assets" column in the dataset can be used to calculate "Margins”, given by Sales Price" - "Assets", can be seen in the total sales/total margins vs. date bar plot. The total margins follow the prices very closely. Daily margins oscillate around the 15% mark (see margins vs. date bar plot), and the Margins vs. Sales Price scatter plot demonstrates that some items do generate losses. In fact this can provide a wealth of information - e.g. one can give preference to products with larger margins in optimisation. The same is true for product classes. Also, given that we have no information about what the actual products and product classes are, this provides one way to make inferences as to their properties and relationships.


USERS FEATURES ANALYSIS:

1)  The analysis depends upon the "hand-crafted" features produced in engineer_features.py. At the moment I've only constructed 7 simple features (see engineer_features.py for details), but many more are certainly possible - the data is rich in this sense at least -  and are necessary for better results.
2) The backbones of a clustering analysis for the customers is performed in analyse_cluster.py. I've so far tried k-means, visualised by reducing the features using PCA. k-means is inadequate for discrete features, such as ”Age" and "Residence Area", so a more sophisticated method is needed. Many more basic features need to be constructed for a conclusive analysis.
3) As an even simpler problem, I've tried to determine whether even the few features engineered so far can be used to predict the age of the users. Supervised learning was used to this end, e.g. the nov_00 data set can be used to train, and feb_02 to test. A simple logistic regression can be used to determine e.g. whether a customer belongs to the economically valuable "35-39" age bracket to 80% accuracy. Softmax applied to all age groups does significantly worse with the current feature set. More sophisticate methods will surely provide much better results, however, it may be that the sparsity issues mentioned above will be a bottleneck ultimately. 


OUTLOOK:

1) Complete the clustering analysis and feature extraction above. I think this is the most promising direction. 
2) Make sure to deal with outliers in data better. So far, I’ve only used a log transformation, which is not adequate.
2) Apply neural nets e.g. to the "35-39" age group problem, and try and understand which features dominate. It is for this reason I used tensorflow from the start, i.e. even for log. regression, since once more features are available it will be easy to generalise the analysis using more sophisticated methods. 
3) Attempt product recommendation analysis. Sparsity of product and user data will be a significant obstacle (see point 5) in 'GENERAL OBSERVATIONS" above). For example, around 30% of the users visit only once, so either one needs to drop these users from the dataset, or perhaps apply some sophisticated inference techniques. Generally speaking, historical purchases data is is not amazing for this dataset.
4) More tentatively: attempt to do some kind of time-series analysis - the span of time might not be sufficient to 



